### Data

Originally, when we planned to do Russian ASR, we planned to use the OpenSLR Russian LibriSpeech (RuLS) dataset to fine-tune an XLSR-Wav2Vec2 model. The OpenSLR Russian LibriSpeech (RuLS) dataset contains 98 hours of Russian speech data from public domain audiobooks. The audio is all recorded in quiet environment with clear speech and no interference from background noise or music. The data is available on the site https://openslr.org/96/. 

However, now that we are considering doing End-To-End Speech Translation using Fairseq, we might also use the CoVost dataset, which is 
a multilingual speech-to-text translation corpus from 11 languages into English. We would use the data for Russian speech to English text. This dataset contains 10.2 hours of training data, 9.0 hours of development data, and 8.2 hours of test data. This is a dataset of read speech data - participants were recorded reading donated sentences. The transcriptions of these sentences were sent to translators to translate into English. There are some sentences that were read out by multiple different speakers, presumably to reduce overfitting to a single speaker. The data is available for download on https://github.com/facebookresearch/covost.

We are also considering using subtitling data as training data. We currently have about 14 hours of a Russian show with manually written, gold standard English subtitles, which we could use as the training data. We already have the video files and subtitles downloaded as they were created by one of our team members a few years ago. We could also find more subtitle data online. In the case of subtitled data, we would need to extract the audio from the video files, and run a script that would separate the one large audio file into small audio files that match the timestamps in the subtitles. In this way, we would create a dataset which contains around one to two sentences in each audio file, with the target being the subtitle transcription. This would be similar to the OpenSLR RuLS dataset. However, this dataset would be much noisier than a read speech dataset. Speech in movies and TV is often combined with background noise and music. One of the interesting things we could investigate in this project would be the effect of noise in the dataset on speech recognition and how it affects WER.

In each case, we will store the data on our personal laptops or on Google Drive.

### Previous Works

In paper [[2] (https://link.springer.com/chapter/10.1007%2F978-3-319-43958-7_13)], Prudnikov et. al. (2016) present the latest improvements to the Russian spontaneous speech recognition system developed in the Speech Technology Center (STC). Spontaneous conversational speech recognition is one of the most difficult tasks in the field of Automatic Speech Recognition, as conversational speech is very noisy and error-prone. Conversational speech varies greatly based on the diversity of individual speakers' speech style, accent, speech rate variability, and the speaker's emotions. It might also contain informal words, such as slang, and might have slurred and poorly articulated words. Background noise and music might also have an effect. There are existing highly effective spontaneous speech recognition systems for English that achieve a WER between 8.0% and 14.1%. In this paper, the goal of the experiment was to build a speaker-independent system for Russian spontaneous speech recognition. This is more challenging for Russian as compared to English for several reasons, two of which are the lack of available datasets, and because Russian is an inflective language with much more unique words than English. The team's previous system achieved a WER of 25.1%, and in this experiment, they improved that rate to 16.4% by using acoustic modelling approaches, combined with deep BLSTM acoustic models and hypothesis rescoring with RNN-based language models. 
